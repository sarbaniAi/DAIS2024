{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6b1efa-d941-4f94-87e7-4a78eba5b41f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Accelerating Speech to text with Fine-Tuning ASR model on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac8a932-bf1c-4c77-911b-d6112d6c9c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Note\n",
    "- This is \"work in progress\" notebook , should be finished before the event\n",
    "- During the event I will present the result of GPU experiments and the accuracy of the Fine Tuned model\n",
    "- Notebook comments will be updated later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "542ce5e5-9efd-4bc7-bc7f-ef3b2151fb3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06b66be-a8f5-4fd6-b561-b64e040eaa07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c68dc028384546be014feb3a057495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ad1688-33ad-46fc-88b8-fd7974d2d148",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30b985eb-aee0-4ef6-805e-ea4f425bac35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using ü§ó Datasets,we download and prepare the Common Voice splits . \n",
    "\n",
    "First, ensure you have accepted the terms of use on the Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally.\n",
    "\n",
    "Since Hindi & Bengali are very low-resource, we'll combine the `train` and `validation` splits to give approximately 8 hours of training data. We'll use the 4 hours of `test` data as our held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2fd097-d1c9-4a8a-b133-bbf873b7c23a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fcb2b3bc524be181da7683f39f58cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90c22db83a74716ba779d6c5c77ddbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d39bc2de4a4e0e9daced5826183e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5661ac4488244e6abfe32e0a0aaaa3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b2667d653c49d09e1c30e8f0052ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b01fbb8f98d4eb8b30f8675a77a38c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595e00c8185d40429b47d63e5a69a95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/96.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57147c65f3ff43d4b8664e1d2ba1588f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e650bb44ec224cf8bb1a9de551ec47d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/89.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9c464ef2c9488789d6659030c450fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/477M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bc89c60c3244fa8ba3fcca5b4b38ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/64.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764d1bfb8f77422d98a18390397b25cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5994b1f353714171895f65e5a7d6d8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa0d28c0a1249f28a5423dbb4e1769e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/787k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72fe91d25a64d5692c911a585504a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/650k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53420028df7464fb7e5c01c4eafb110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/636k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e16a125060c48cead3ba7f374748c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0023d55ce64d8e917adadf7635e8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/501k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ca4e6b0821449d8a1c3fbe827e008b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff8af03eb5843c58d952a5dc9213867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rReading metadata...: 0it [00:00, ?it/s]\u001B[A\rReading metadata...: 2677it [00:00, 151414.63it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a650867391334149b47d9eedf292c530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rReading metadata...: 0it [00:00, ?it/s]\u001B[A\rReading metadata...: 2227it [00:00, 151787.76it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740ab2d2fdcb40c6af4bef5b96470273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rReading metadata...: 0it [00:00, ?it/s]\u001B[A\rReading metadata...: 2212it [00:00, 150691.93it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd91e3578a24eefab17c4663bf21dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating other split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rReading metadata...: 0it [00:00, ?it/s]\u001B[A\n\rReading metadata...: 15252it [00:00, 152508.51it/s]\u001B[A\rReading metadata...: 16395it [00:00, 149913.48it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fd76dda7264abf84c3c5c641fc0c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating invalidated split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rReading metadata...: 0it [00:00, ?it/s]\u001B[A\rReading metadata...: 1653it [00:00, 150754.18it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n    train: Dataset({\n        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n        num_rows: 4904\n    })\n    test: Dataset({\n        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n        num_rows: 2212\n    })\n})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14adf9c3-fd95-4c44-9e80-387127d40213",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3afdb16d-8b66-456e-9e1e-f979381e6890",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 05:41:35.721997: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-01-04 05:41:35.766262: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-01-04 05:41:35.766302: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-01-04 05:41:35.766329: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-01-04 05:41:35.775375: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'english': 'en',\n",
       " 'chinese': 'zh',\n",
       " 'german': 'de',\n",
       " 'spanish': 'es',\n",
       " 'russian': 'ru',\n",
       " 'korean': 'ko',\n",
       " 'french': 'fr',\n",
       " 'japanese': 'ja',\n",
       " 'portuguese': 'pt',\n",
       " 'turkish': 'tr',\n",
       " 'polish': 'pl',\n",
       " 'catalan': 'ca',\n",
       " 'dutch': 'nl',\n",
       " 'arabic': 'ar',\n",
       " 'swedish': 'sv',\n",
       " 'italian': 'it',\n",
       " 'indonesian': 'id',\n",
       " 'hindi': 'hi',\n",
       " 'finnish': 'fi',\n",
       " 'vietnamese': 'vi',\n",
       " 'hebrew': 'he',\n",
       " 'ukrainian': 'uk',\n",
       " 'greek': 'el',\n",
       " 'malay': 'ms',\n",
       " 'czech': 'cs',\n",
       " 'romanian': 'ro',\n",
       " 'danish': 'da',\n",
       " 'hungarian': 'hu',\n",
       " 'tamil': 'ta',\n",
       " 'norwegian': 'no',\n",
       " 'thai': 'th',\n",
       " 'urdu': 'ur',\n",
       " 'croatian': 'hr',\n",
       " 'bulgarian': 'bg',\n",
       " 'lithuanian': 'lt',\n",
       " 'latin': 'la',\n",
       " 'maori': 'mi',\n",
       " 'malayalam': 'ml',\n",
       " 'welsh': 'cy',\n",
       " 'slovak': 'sk',\n",
       " 'telugu': 'te',\n",
       " 'persian': 'fa',\n",
       " 'latvian': 'lv',\n",
       " 'bengali': 'bn',\n",
       " 'serbian': 'sr',\n",
       " 'azerbaijani': 'az',\n",
       " 'slovenian': 'sl',\n",
       " 'kannada': 'kn',\n",
       " 'estonian': 'et',\n",
       " 'macedonian': 'mk',\n",
       " 'breton': 'br',\n",
       " 'basque': 'eu',\n",
       " 'icelandic': 'is',\n",
       " 'armenian': 'hy',\n",
       " 'nepali': 'ne',\n",
       " 'mongolian': 'mn',\n",
       " 'bosnian': 'bs',\n",
       " 'kazakh': 'kk',\n",
       " 'albanian': 'sq',\n",
       " 'swahili': 'sw',\n",
       " 'galician': 'gl',\n",
       " 'marathi': 'mr',\n",
       " 'punjabi': 'pa',\n",
       " 'sinhala': 'si',\n",
       " 'khmer': 'km',\n",
       " 'shona': 'sn',\n",
       " 'yoruba': 'yo',\n",
       " 'somali': 'so',\n",
       " 'afrikaans': 'af',\n",
       " 'occitan': 'oc',\n",
       " 'georgian': 'ka',\n",
       " 'belarusian': 'be',\n",
       " 'tajik': 'tg',\n",
       " 'sindhi': 'sd',\n",
       " 'gujarati': 'gu',\n",
       " 'amharic': 'am',\n",
       " 'yiddish': 'yi',\n",
       " 'lao': 'lo',\n",
       " 'uzbek': 'uz',\n",
       " 'faroese': 'fo',\n",
       " 'haitian creole': 'ht',\n",
       " 'pashto': 'ps',\n",
       " 'turkmen': 'tk',\n",
       " 'nynorsk': 'nn',\n",
       " 'maltese': 'mt',\n",
       " 'sanskrit': 'sa',\n",
       " 'luxembourgish': 'lb',\n",
       " 'myanmar': 'my',\n",
       " 'tibetan': 'bo',\n",
       " 'tagalog': 'tl',\n",
       " 'malagasy': 'mg',\n",
       " 'assamese': 'as',\n",
       " 'tatar': 'tt',\n",
       " 'hawaiian': 'haw',\n",
       " 'lingala': 'ln',\n",
       " 'hausa': 'ha',\n",
       " 'bashkir': 'ba',\n",
       " 'javanese': 'jw',\n",
       " 'sundanese': 'su',\n",
       " 'burmese': 'my',\n",
       " 'valencian': 'ca',\n",
       " 'flemish': 'nl',\n",
       " 'haitian': 'ht',\n",
       " 'letzeburgesch': 'lb',\n",
       " 'pushto': 'ps',\n",
       " 'panjabi': 'pa',\n",
       " 'moldavian': 'ro',\n",
       " 'moldovan': 'ro',\n",
       " 'sinhalese': 'si',\n",
       " 'castilian': 'es'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "\n",
    "TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1414146f-26cf-4913-b00a-d207d5a6d147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e851a718edf43eeb004f2954568c717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ce5d796cb144339e7b06dda267e2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4505c44f6ff46a1bcc9804368494e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7925cf6d66764031af3ad9b14271e483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2c0ebd0e3c4466b9f0e879acb034b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27507fcc315d4bdeb5c29fdfaead6d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9481dfab3e0642828968356010840343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a228017e6f4a4593bbd8a11a80d57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"sinhalese\", task=\"transcribe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2be948-243c-46e7-b969-30aec7abf0f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
       " 'sentence': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87f302e-c277-406c-8c98-b652a589ad85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da22c24-88c5-4d53-ae4f-561863e09838",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        text=example[\"sentence\"],\n",
    "    )\n",
    "\n",
    "    # compute input length of audio sample in seconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14294c0c-33e2-4183-9f81-a5ff962314b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635b12bbce6f4e0c8746acb7963781cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54268a8d45b44003988cdedbca74c0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3fbefa-7f4e-4896-86c7-78081d8b9642",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16fb07c-a7a2-4de2-85df-cd07b74f687d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9000d42cdc2241a990d78a12218a7378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c70a24-7364-45ad-9ea3-b6a8c862e9f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Prepare Feature Extractor, Tokenizer and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56815f18-dae1-4fe4-9115-daec0662b9e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
    "\n",
    "- Evaluation metrics: during evaluation, we want to evaluate the model using the [word error rate (WER) metric. We need to define a `compute_metrics` function that handles this computation.\n",
    "\n",
    "- Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "- Define the training configuration: this will be used by the ü§ó Trainer to define the training schedule.\n",
    "\n",
    "Once we've fine-tuned the model, we will evaluate it on the test data to verify that we have correctly trained it \n",
    "to transcribe speech in Hindi & Bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb8636e-be5d-4f9f-9bf1-14fbf671e804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760ee30b-0fe5-41b1-b8bf-e0662c1148da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "780f52f9-a727-42ca-bfef-9fa629f1ce89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define Evaluation metrics: \n",
    "We want to evaluate the model using the Word Error Rate (WER) metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20f8de7-aa45-4b5a-99d4-d95a682523c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting jiwer\n  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\nCollecting rapidfuzz<4,>=3\n  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.4/3.4 MB 41.7 MB/s eta 0:00:00\nCollecting click<9.0.0,>=8.1.3\n  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 97.9/97.9 kB 20.9 MB/s eta 0:00:00\nInstalling collected packages: rapidfuzz, click, jiwer\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1db6a16f-326c-47ae-94ec-820da59c304f\n    Can't uninstall 'click'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-store 0.16.2 requires pyspark<4,>=3.1.2, which is not installed.\nSuccessfully installed click-8.1.7 jiwer-3.0.3 rapidfuzz-3.6.1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a2437b-9529-489e-b568-8236ec74bc28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec9d98a406b40f094c3043347da58a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c16e8f6-a7ec-4fde-b0b5-fd751c72fc5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # compute orthographic wer\n",
    "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # compute normalised WER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b27317e-d36d-4372-bb1e-76a1bd118034",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91affd0a344d454faa9862e82d92f980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7fe28b8c04469c84122b402a861341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3523c56a14445e888e49acc727dc083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39597657-c27f-44f7-8f31-6fcd0ae29ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f331a843-31b4-479e-822e-7722c143e094",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### GPU Requirements:\n",
    "\n",
    "Experimenting with various GPU configurations on Databricks AWS ..( In Progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc258a56-0ebc-4ec5-9945-9abc941eeb6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-dv\",  # name on the HF Hub\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b097ab44-8ae6-4335-b000-3b71bafa89da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e330c7-2e76-43c0-ba52-6dc6a36eab8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c38561-4b8c-4f57-be08-8133b5845b02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/torch/nn/modules/conv.py:309: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 44:15, Epoch 1.63/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer Ortho</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.167725</td>\n",
       "      <td>62.023818</td>\n",
       "      <td>12.727336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/serialization.py:441\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n",
       "\u001B[1;32m    440\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n",
       "\u001B[0;32m--> 441\u001B[0m     \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/serialization.py:668\u001B[0m, in \u001B[0;36m_save\u001B[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001B[0m\n",
       "\u001B[1;32m    667\u001B[0m num_bytes \u001B[38;5;241m=\u001B[39m storage\u001B[38;5;241m.\u001B[39mnbytes()\n",
       "\u001B[0;32m--> 668\u001B[0m \u001B[43mzip_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_ptr\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "\u001B[0;31mRuntimeError\u001B[0m: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/187: file write failed\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1312111372204891>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:571\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    569\u001B[0m     patch_function\u001B[38;5;241m.\u001B[39mcall(call_original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    570\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 571\u001B[0m     \u001B[43mpatch_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcall_original\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    573\u001B[0m session\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msucceeded\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    575\u001B[0m try_log_autologging_event(\n",
       "\u001B[1;32m    576\u001B[0m     AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_patch_function_success,\n",
       "\u001B[1;32m    577\u001B[0m     session,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    581\u001B[0m     kwargs,\n",
       "\u001B[1;32m    582\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/transformers/__init__.py:2703\u001B[0m, in \u001B[0;36mautolog.<locals>.train\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   2701\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m   2702\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m disable_discrete_autologging(DISABLED_ANCILLARY_FLAVOR_AUTOLOGGING):\n",
       "\u001B[0;32m-> 2703\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moriginal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:552\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    549\u001B[0m         original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n",
       "\u001B[1;32m    550\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\u001B[0;32m--> 552\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall_original_fn_with_event_logging\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_original_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mog_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mog_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:487\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    479\u001B[0m     try_log_autologging_event(\n",
       "\u001B[1;32m    480\u001B[0m         AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_original_function_start,\n",
       "\u001B[1;32m    481\u001B[0m         session,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    485\u001B[0m         og_kwargs,\n",
       "\u001B[1;32m    486\u001B[0m     )\n",
       "\u001B[0;32m--> 487\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mog_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mog_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    489\u001B[0m     try_log_autologging_event(\n",
       "\u001B[1;32m    490\u001B[0m         AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_original_function_success,\n",
       "\u001B[1;32m    491\u001B[0m         session,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    495\u001B[0m         og_kwargs,\n",
       "\u001B[1;32m    496\u001B[0m     )\n",
       "\u001B[1;32m    497\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:549\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001B[0;34m(*_og_args, **_og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    541\u001B[0m \u001B[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001B[39;00m\n",
       "\u001B[1;32m    542\u001B[0m \u001B[38;5;66;03m# during original function execution, even if silent mode is enabled\u001B[39;00m\n",
       "\u001B[1;32m    543\u001B[0m \u001B[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001B[39;00m\n",
       "\u001B[1;32m    544\u001B[0m \u001B[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001B[39;00m\n",
       "\u001B[1;32m    545\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n",
       "\u001B[1;32m    546\u001B[0m     disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    547\u001B[0m     reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    548\u001B[0m ):\n",
       "\u001B[0;32m--> 549\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_og_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_og_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    550\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/huggingface_patches/transformers.py:54\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_fit_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     52\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 54\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     55\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:1582\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1579\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1580\u001B[0m     \u001B[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001B[39;00m\n",
       "\u001B[1;32m   1581\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39mdisable_progress_bars()\n",
       "\u001B[0;32m-> 1582\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1583\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1584\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1585\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1586\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1587\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1588\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1589\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:1984\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n",
       "\u001B[1;32m   1981\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m steps_skipped) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n",
       "\u001B[1;32m   1982\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
       "\u001B[0;32m-> 1984\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1985\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1986\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_substep_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2339\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n",
       "\u001B[1;32m   2336\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mstep(metrics[metric_to_check])\n",
       "\u001B[1;32m   2338\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_save:\n",
       "\u001B[0;32m-> 2339\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2340\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_save(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2396\u001B[0m, in \u001B[0;36mTrainer._save_checkpoint\u001B[0;34m(self, model, trial, metrics)\u001B[0m\n",
       "\u001B[1;32m   2394\u001B[0m run_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_output_dir(trial\u001B[38;5;241m=\u001B[39mtrial)\n",
       "\u001B[1;32m   2395\u001B[0m output_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(run_dir, checkpoint_folder)\n",
       "\u001B[0;32m-> 2396\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_internal_call\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_deepspeed_enabled:\n",
       "\u001B[1;32m   2398\u001B[0m     \u001B[38;5;66;03m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001B[39;00m\n",
       "\u001B[1;32m   2399\u001B[0m     \u001B[38;5;66;03m# config `stage3_gather_16bit_weights_on_model_save` is True\u001B[39;00m\n",
       "\u001B[1;32m   2400\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped\u001B[38;5;241m.\u001B[39msave_checkpoint(output_dir)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2901\u001B[0m, in \u001B[0;36mTrainer.save_model\u001B[0;34m(self, output_dir, _internal_call)\u001B[0m\n",
       "\u001B[1;32m   2898\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped\u001B[38;5;241m.\u001B[39msave_checkpoint(output_dir)\n",
       "\u001B[1;32m   2900\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mshould_save:\n",
       "\u001B[0;32m-> 2901\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2903\u001B[0m \u001B[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001B[39;00m\n",
       "\u001B[1;32m   2904\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpush_to_hub \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _internal_call:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2959\u001B[0m, in \u001B[0;36mTrainer._save\u001B[0;34m(self, output_dir, state_dict)\u001B[0m\n",
       "\u001B[1;32m   2957\u001B[0m             torch\u001B[38;5;241m.\u001B[39msave(state_dict, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_dir, WEIGHTS_NAME))\n",
       "\u001B[1;32m   2958\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 2959\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   2960\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe_serialization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_safetensors\u001B[49m\n",
       "\u001B[1;32m   2961\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2964\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(output_dir)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/modeling_utils.py:2114\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   2112\u001B[0m         safe_save_file(shard, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, shard_file), metadata\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
       "\u001B[1;32m   2113\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 2114\u001B[0m         \u001B[43msave_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2117\u001B[0m     path_to_weights \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, _add_variant(WEIGHTS_NAME, variant))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/serialization.py:440\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n",
       "\u001B[1;32m    437\u001B[0m _check_save_filelike(f)\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n",
       "\u001B[0;32m--> 440\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n",
       "\u001B[1;32m    441\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
       "\u001B[1;32m    442\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/serialization.py:291\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__exit__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m--> 291\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_like\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_end_of_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "\u001B[0;31mRuntimeError\u001B[0m: [enforce fail at inline_container.cc:337] . unexpected pos 185095872 vs 185095760"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/serialization.py:441\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[0;32m--> 441\u001B[0m     \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/serialization.py:668\u001B[0m, in \u001B[0;36m_save\u001B[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001B[0m\n\u001B[1;32m    667\u001B[0m num_bytes \u001B[38;5;241m=\u001B[39m storage\u001B[38;5;241m.\u001B[39mnbytes()\n\u001B[0;32m--> 668\u001B[0m \u001B[43mzip_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_ptr\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bytes\u001B[49m\u001B[43m)\u001B[49m\n\n\u001B[0;31mRuntimeError\u001B[0m: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/187: file write failed\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\nFile \u001B[0;32m<command-1312111372204891>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:571\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    569\u001B[0m     patch_function\u001B[38;5;241m.\u001B[39mcall(call_original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    570\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 571\u001B[0m     \u001B[43mpatch_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcall_original\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    573\u001B[0m session\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msucceeded\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    575\u001B[0m try_log_autologging_event(\n\u001B[1;32m    576\u001B[0m     AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_patch_function_success,\n\u001B[1;32m    577\u001B[0m     session,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    581\u001B[0m     kwargs,\n\u001B[1;32m    582\u001B[0m )\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/transformers/__init__.py:2703\u001B[0m, in \u001B[0;36mautolog.<locals>.train\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2701\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   2702\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m disable_discrete_autologging(DISABLED_ANCILLARY_FLAVOR_AUTOLOGGING):\n\u001B[0;32m-> 2703\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moriginal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:552\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n\u001B[1;32m    549\u001B[0m         original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n\u001B[1;32m    550\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n\u001B[0;32m--> 552\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall_original_fn_with_event_logging\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_original_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mog_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mog_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:487\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    479\u001B[0m     try_log_autologging_event(\n\u001B[1;32m    480\u001B[0m         AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_original_function_start,\n\u001B[1;32m    481\u001B[0m         session,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         og_kwargs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mog_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mog_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    489\u001B[0m     try_log_autologging_event(\n\u001B[1;32m    490\u001B[0m         AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_original_function_success,\n\u001B[1;32m    491\u001B[0m         session,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    495\u001B[0m         og_kwargs,\n\u001B[1;32m    496\u001B[0m     )\n\u001B[1;32m    497\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:549\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001B[0;34m(*_og_args, **_og_kwargs)\u001B[0m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001B[39;00m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;66;03m# during original function execution, even if silent mode is enabled\u001B[39;00m\n\u001B[1;32m    543\u001B[0m \u001B[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001B[39;00m\n\u001B[1;32m    544\u001B[0m \u001B[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001B[39;00m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001B[1;32m    546\u001B[0m     disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    547\u001B[0m     reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    548\u001B[0m ):\n\u001B[0;32m--> 549\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_og_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_og_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    550\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/huggingface_patches/transformers.py:54\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_fit_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 54\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:1582\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1579\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1580\u001B[0m     \u001B[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001B[39;00m\n\u001B[1;32m   1581\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39mdisable_progress_bars()\n\u001B[0;32m-> 1582\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1583\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1584\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1585\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1586\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1587\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1588\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1589\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:1984\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1981\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m steps_skipped) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n\u001B[1;32m   1982\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m-> 1984\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1985\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1986\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_substep_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2339\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2336\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mstep(metrics[metric_to_check])\n\u001B[1;32m   2338\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[0;32m-> 2339\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2340\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_save(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2396\u001B[0m, in \u001B[0;36mTrainer._save_checkpoint\u001B[0;34m(self, model, trial, metrics)\u001B[0m\n\u001B[1;32m   2394\u001B[0m run_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_output_dir(trial\u001B[38;5;241m=\u001B[39mtrial)\n\u001B[1;32m   2395\u001B[0m output_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(run_dir, checkpoint_folder)\n\u001B[0;32m-> 2396\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_internal_call\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   2397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_deepspeed_enabled:\n\u001B[1;32m   2398\u001B[0m     \u001B[38;5;66;03m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001B[39;00m\n\u001B[1;32m   2399\u001B[0m     \u001B[38;5;66;03m# config `stage3_gather_16bit_weights_on_model_save` is True\u001B[39;00m\n\u001B[1;32m   2400\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped\u001B[38;5;241m.\u001B[39msave_checkpoint(output_dir)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2901\u001B[0m, in \u001B[0;36mTrainer.save_model\u001B[0;34m(self, output_dir, _internal_call)\u001B[0m\n\u001B[1;32m   2898\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped\u001B[38;5;241m.\u001B[39msave_checkpoint(output_dir)\n\u001B[1;32m   2900\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[0;32m-> 2901\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2903\u001B[0m \u001B[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001B[39;00m\n\u001B[1;32m   2904\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpush_to_hub \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _internal_call:\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/trainer.py:2959\u001B[0m, in \u001B[0;36mTrainer._save\u001B[0;34m(self, output_dir, state_dict)\u001B[0m\n\u001B[1;32m   2957\u001B[0m             torch\u001B[38;5;241m.\u001B[39msave(state_dict, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_dir, WEIGHTS_NAME))\n\u001B[1;32m   2958\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2959\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2960\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe_serialization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_safetensors\u001B[49m\n\u001B[1;32m   2961\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2964\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(output_dir)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/modeling_utils.py:2114\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[1;32m   2112\u001B[0m         safe_save_file(shard, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, shard_file), metadata\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[1;32m   2113\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2114\u001B[0m         \u001B[43msave_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2117\u001B[0m     path_to_weights \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, _add_variant(WEIGHTS_NAME, variant))\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/serialization.py:440\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n\u001B[1;32m    437\u001B[0m _check_save_filelike(f)\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[0;32m--> 440\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m    441\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001B[1;32m    442\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/serialization.py:291\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__exit__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 291\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_like\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_end_of_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\n\u001B[0;31mRuntimeError\u001B[0m: [enforce fail at inline_container.cc:337] . unexpected pos 185095872 vs 185095760",
       "errorSummary": "<span class='ansi-red-fg'>RuntimeError</span>: [enforce fail at inline_container.cc:337] . unexpected pos 185095872 vs 185095760",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60434d60-bf0f-4bbd-b3d9-19f5ac3edfc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
    "    \"dataset\": \"Common Voice 13\",  # a 'pretty' name for the training dataset\n",
    "    \"language\": \"dv\",\n",
    "    \"model_name\": \"Whisper Small Dv - Sarbani Maiti\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25e8c9d-68af-409b-8cf7-edb93213057d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d56684-de39-4211-902d-a5783517d59e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### MLFLOW Integration - To be coded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd3f6a8-09c3-4c02-a9fc-88978417850c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "testpath = \"Enter Volume Path\"\n",
    "with mlflow.start_run() as run:\n",
    "  trainer.train()\n",
    "  trainer.save_model(model_output_dir)\n",
    "  #pipe = pipeline(\"text-classification\", model=AutoModelForSequenceClassification.from_pretrained(model_output_dir), batch_size=1, tokenizer=tokenizer)\n",
    "  pipe = pipeline(model=\"sarbani-maiti/whisper-small-hi\")  \n",
    "  model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=pipe,\n",
    "        artifact_path=\"ASRModel\",\n",
    "        input_example=\"testpath/test.mp3\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01ec07a-a95d-4abd-afbc-787ae402a096",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14b9ff56-150f-4d2e-9ef8-494f8c4b9b1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b63d12df-63ba-46cc-b325-6f09be0a5c02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c7d28b8-adb6-44a0-a82b-f83b3869d89d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a71bfc1e-dab7-47a0-8868-37b5a40eea8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13109c2-f2a8-4c25-ab3d-bdaa2c9d4968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### End to end Demo with Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc71fb67-a750-464e-b315-fa283d632bc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"sarbani-maiti/whisper-small-hi\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Sarbani Maiti_Acceleratxaing Speech to text with Fine-Tuning ASR model on Databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
